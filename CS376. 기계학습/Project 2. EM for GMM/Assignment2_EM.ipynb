{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MrCD3KOcA7b"
      },
      "source": [
        "In the first part of this assignment, we will implement Gaussian Mixture Model and Expectation Maximization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ1bzM99cA7c"
      },
      "source": [
        "### **Preparation**\n",
        "The following code is the preparation for importing packages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "from sklearn.cluster import KMeans\n",
        "np.random.seed(717)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR1CQd2acA7d"
      },
      "source": [
        "### **Gaussian mixture model and Expectation maximization Implementation**\n",
        "\n",
        "In this assignment, you should use only NumPy to build the Gaussian mixture models.\n",
        "\n",
        "**DO NOT** use other libraries to implement the Gaussian mixture models.\n",
        "\n",
        "**DO NOT** modify other parts of the skeleton code.\n",
        "\n",
        "Follow the comments. They'll give you instructions on what to code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uryEngU8cA7e"
      },
      "source": [
        "### Step 1. Implement EM for GMM\n",
        "The followings are the skeleton codes. Implement the functions inside the GMM class.\n",
        "\n",
        "Use the pre-imported function  ```mvn(mean, cov)``` to calculate the probability density function of multivariate Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n0kK7i_bcA7e"
      },
      "outputs": [],
      "source": [
        "class GMM():\n",
        "    def __init__(self, X_data, num_cluster):\n",
        "        # Initialize the model parameters as the instance variable.\n",
        "        self.N, self.dim = X_data.shape\n",
        "        self.K = num_cluster\n",
        "        self.means = np.random.random((self.K, self.dim))\n",
        "        self.covariances = np.array([np.eye(self.dim)] * self.K)\n",
        "        self.mixing_coefficients = np.random.random(self.K)\n",
        "        self.mixing_coefficients /= self.mixing_coefficients.sum()\n",
        "        self.gammas = np.zeros((self.K, self.N)) # ð›¾_ð‘˜(ð‘›)\n",
        "    \n",
        "    # Function for computing log likelihood.\n",
        "    def log_likelihood(self, X_data):\n",
        "        result = 0\n",
        "        for i in range(self.N):\n",
        "            # Initialize the probability as zero.\n",
        "            prob = 0\n",
        "            for j in range(self.K):\n",
        "                # Multiply the probability density function with the mixing coefficient.\n",
        "                prob += self.mixing_coefficients[j] * mvn(self.means[j], self.covariances[j]).pdf(X_data[i])\n",
        "            # Add the log-likelihood values to the output.\n",
        "            result += np.log(prob)\n",
        "        return result\n",
        "\n",
        "    # Function for expectation step.\n",
        "    def E_step(self, X_data):\n",
        "        # Compute the posterior probability.\n",
        "        for i in range(self.K):\n",
        "            for j in range(self.N):\n",
        "                # In order to adgust the parameter,Compute the posterior probability over all possibilities.\n",
        "                self.gammas[i,j] = self.mixing_coefficients[i] * mvn(self.means[i], self.covariances[i]).pdf(X_data[j])\n",
        "        # Normalize the posterior probability for each data point.\n",
        "        self.gammas /= self.gammas.sum(0)\n",
        "\n",
        "    # Function for maximization step.\n",
        "    def M_step(self, X_data):\n",
        "        self.mixing_coefficients = np.zeros(self.K)\n",
        "        self.means = np.zeros((self.dim, self.dim))\n",
        "        \n",
        "        # Update the mixing coefficient vector using the result of the E step.\n",
        "        for i in range(self.K):\n",
        "            for j in range(self.N):\n",
        "                self.mixing_coefficients[i] += self.gammas[i, j]\n",
        "        # Normalize the mixing coefficient.\n",
        "        self.mixing_coefficients /= self.N\n",
        "\n",
        "        # Update the mean vector.\n",
        "        for i in range(self.K):\n",
        "            for j in range(self.N):\n",
        "                # Multiply the each data point by the corresponding ð›¾_ð‘˜(ð‘›).\n",
        "                self.means[i] += self.gammas[i, j] * X_data[j]\n",
        "            # Divide by the number of data points in the i-th distribution\n",
        "            self.means[i] /= self.gammas[i, :].sum()\n",
        "\n",
        "        # Update the covariance matrix.\n",
        "        for i in range(self.K):\n",
        "            for j in range(self.N):\n",
        "                # Subtract mean from the data point.\n",
        "                tmp = np.reshape(X_data[j] - self.means[i], (len(self.means),1))\n",
        "                # Multiply the above matrix and multiply the gamma value.\n",
        "                self.covariances[i] += self.gammas[i,j] * np.dot(tmp, tmp.T)\n",
        "            # Divide by the number of data points in the i-th distribution.\n",
        "            self.covariances[i] /= self.gammas[i,:].sum()\n",
        "\n",
        "    \n",
        "    def train(self, X_data, iterations=1000, threshold=0.001):\n",
        "        # Initialize the log-likelihood value to start the training.\n",
        "        self.P_new = self.log_likelihood(X_data)\n",
        "        self.P_old = 2 * self.P_new # To prevent the immediate termination of the iteration\n",
        "        iteration = 0\n",
        "        while((abs((self.P_old - self.P_new)/self.P_new)*100) > threshold and (iteration <= iterations)):\n",
        "            # Repeat the E and M step to update the parameters\n",
        "            # until the difference in log-likelihood value between iterations, or total iterations end.\n",
        "            self.E_step(X_data)\n",
        "            self.M_step(X_data)\n",
        "            self.P_old = self.P_new\n",
        "            self.P_new = self.log_likelihood(X_data)\n",
        "            iteration += 1\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ol0kuPuQcA7f"
      },
      "outputs": [],
      "source": [
        "def plot_for_EM(X_data, E, mean, mu_1, mu_2, num_1):\n",
        "    # Generates the empty lists for each cluster.\n",
        "    cl1, cl2 = list(), list()\n",
        "\n",
        "    # We don't know the order of the points (due to the initialization),\n",
        "    # so we make two variables (true1, true2) for count\n",
        "    true1 = 0\n",
        "    true2 = 0\n",
        "    for i in range(len(X_data)):\n",
        "        if E[i, 0] > E[i, 1]:\n",
        "            cl1.append(X_data[i,:])\n",
        "            if i < num_1:\n",
        "                true1 += 1\n",
        "            else:\n",
        "                true2 += 1\n",
        "        else:\n",
        "            cl2.append(X_data[i,:])\n",
        "            if i >= num_1:\n",
        "                true1 += 1\n",
        "            else:\n",
        "                true2 += 1\n",
        "                \n",
        "    # Make lists into the NumPy array.\n",
        "    cl1 = np.array(cl1)\n",
        "    cl2 = np.array(cl2)\n",
        "    index = 0 if true1 > true2 else 1\n",
        "    color = ['b.', 'r.']\n",
        "    \n",
        "    # Compute the accuracy of the prediction for the EM algorithm and the center of each cluster.\n",
        "    print(f'- The accuracy of the prediction for the EM algorithm : {float(max(true1, true2)) / len(X_data)}')\n",
        "    print(f'- The estimated center of cluster 1 : ({mean[index,0]}, {mean[index,1]})')\n",
        "    print(f'- The estimated center of cluster 2 : ({mean[1-index,0]}, {mean[1-index,1]})')\n",
        "    \n",
        "    fig = plt.figure(figsize=(9, 6), dpi=100)\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.scatter(cl1[:,0], cl1[:,1], label=\"Cluster 1\")\n",
        "    ax.scatter(cl2[:,0], cl2[:,1], label=\"Cluster 2\")\n",
        "    ax.plot(mean[0,0], mean[0,1], marker='*', color='black', linestyle='None',label=\"Estimated center\")\n",
        "    ax.plot(mean[1,0], mean[1,1], marker='*', color='black', linestyle='None')\n",
        "    ax.plot(mu_1[0], mu_1[1], marker='+', color='black', linestyle='None', label=\"Ground truth center\")\n",
        "    ax.plot(mu_2[0], mu_2[1], marker='+', color='black', linestyle='None')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(\"The scatter plot of the prediction for EM algorithm\", fontsize=17)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d6getoKcA7f"
      },
      "source": [
        "### Step 2. Load the sample dataset\n",
        "Load the sample dataset and visualize it using the scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnYZ5BH5cCUw",
        "outputId": "fff34d37-09d9-4600-83ad-88891e6ba8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEd5FDrncA7f"
      },
      "outputs": [],
      "source": [
        "# The Dataset(X_data).npy is generated by the multivariate gaussian distribution.\n",
        "# To compute the accuracy and visualize the results, the graound truth information is as follows: \n",
        "mu_1 = [0, 4] # The mean vector of each group.\n",
        "mu_2 = [-2, 0] # The mean vector of each group.\n",
        "cluster_idx = [600, 400] # The number of samples of each group.\n",
        "num_cluster = len(cluster_idx)\n",
        "X_data = np.load(\"./drive/MyDrive/Dataset(X_data).npy\")\n",
        "\n",
        "fig = plt.figure(figsize=(9, 6), dpi=100)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(X_data[:,0], X_data[:,1], label=\"Sample dataset\")\n",
        "plt.legend()\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(\"The scatter plot of the sample dataset\", fontsize=17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2NndG5AcA7g"
      },
      "source": [
        "### Step 3. Run the EM algorithm on the sample dataset and visualize the prediction results\n",
        "Initialize the GMM model with the sample data and the number of samples or each cluster. Set the total iterations and threshold to complete learning, and then run the EM algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A3CqalTHcA7g"
      },
      "outputs": [],
      "source": [
        "model = GMM(X_data, num_cluster = num_cluster)\n",
        "model.train(X_data, iterations = 1000, threshold=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOLvMoiQcA7g"
      },
      "outputs": [],
      "source": [
        "plot_for_EM(X_data, model.gammas.T, model.means, mu_1, mu_2, cluster_idx[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HjBlrvecA7g"
      },
      "source": [
        "### Step 4.  Visualize the prediction result of the K-Means algorithm for the sample dataset\n",
        "Check the difference in the predicted center coordinates between the EM algorithm and the K-Means algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZChnd9mtcA7g"
      },
      "outputs": [],
      "source": [
        "# Run KMean Clustering\n",
        "kmeans = KMeans(n_clusters = 2, n_init = 10)\n",
        "kmeans.fit(X_data)\n",
        "labels = kmeans.predict(X_data)\n",
        "kmean_center = kmeans.cluster_centers_\n",
        "\n",
        "x = X_data[:, 0]\n",
        "y = X_data[:, 1]\n",
        "x_1, y_1 = x[labels==0], y[labels==0]\n",
        "x_2, y_2 = x[labels==1], y[labels==1]\n",
        "m_1 = np.array([np.average(x_1), np.average(y_1)])\n",
        "m_2 = np.array([np.average(x_2), np.average(y_2)])\n",
        "\n",
        "# We don't know the order of the points (due to the initialization), so we check the distance between center and the mean\n",
        "true = 0.0\n",
        "num = cluster_idx[0]\n",
        "index = 0 if np.linalg.norm(m_1 - mu_1) < np.linalg.norm(m_1 - mu_2) else 1\n",
        "true += len(labels[:num][labels[:num]==index])\n",
        "true += len(labels[num:][labels[num:]==1 - index])\n",
        "color = ['b.', 'r.']\n",
        "\n",
        "# Compute the accuracy of the prediction for the N-Means algorithm and the center of each cluster.\n",
        "print(f'- The accuracy of the prediction for the K-Means algorithm : {true / len(X_data)}')\n",
        "print(f'- The estimated center of cluster 1 : ({kmean_center[index,0]}, {kmean_center[index,1]})')\n",
        "print(f'- The estimated center of cluster 2 : ({kmean_center[1-index,0]}, {kmean_center[1-index,1]})')\n",
        "\n",
        "fig = plt.figure(figsize=(9, 6), dpi=100)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(x_1, y_1,label=\"Cluster 1\")\n",
        "ax.scatter(x_2, y_2, label=\"Cluster 2\")\n",
        "ax.plot(m_1[0], m_1[1], marker='*', color='black', linestyle='None',label=\"Estimated center\")\n",
        "ax.plot(m_2[0], m_2[1], marker='*', color='black', linestyle='None')\n",
        "ax.plot(mu_1[0], mu_1[1], marker='+', color='black', linestyle='None', label=\"Ground truth center\")\n",
        "ax.plot(mu_2[0], mu_2[1], marker='+', color='black', linestyle='None')\n",
        "plt.legend()\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(\"The scatter plot of the prediction for K-Means algorithm\", fontsize=17)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
